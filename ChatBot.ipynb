{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ChatBot.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP30seTtFcYgvQOeNidsOHw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaeSeokSong/MachineLearningModels/blob/main/ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96hYYUFzcaxY"
      },
      "source": [
        "# Google Drive Local Mount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHqDHk2ucc7M"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgHeq1Zcb7M2"
      },
      "source": [
        "# Install Morpheme analyzer\n",
        "*   [Reference](https://soohee410.github.io/compare_tagger)\n",
        "*   [Install](https://sanghyu.tistory.com/170)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtYcWqVsb5Xk"
      },
      "source": [
        "# okt, komoran, kkma\n",
        "# install konlpy (okt, komoran, kkma)\n",
        "%%bash\n",
        "apt-get update\n",
        "apt-get install g++ openjdk-8-jdk python-dev python3-dev\n",
        "pip3 install JPype1\n",
        "pip3 install konlpy\n",
        "\n",
        "# mecab (take a long time)\n",
        "# set env\n",
        "%env JAVA_HOME \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "# install konlpy (mecab)\n",
        "%%bash\n",
        "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
        "pip3 install /tmp/mecab-python-0.996"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GobPxVERuegZ"
      },
      "source": [
        "# Error Solution\n",
        "\n",
        "1. TypeError: startJVM() got an unexpected keyword argument 'convertStrings' [(JVM)](https://gyulogs.tistory.com/130)\n",
        "2. NameError: name 'Tagger' is not defined [(Mecab)](https://sosomemo.tistory.com/31)\n",
        "3. ParserError: Error tokenizing data. C error [(Pandas)](https://mskim8717.tistory.com/82)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpM2dDEuuesD"
      },
      "source": [
        "\"\"\" \n",
        "ERROR 1. TypeError: startJVM() got an unexpected keyword argument 'convertStrings'\n",
        "- Solution: /usr/local/lib/python3.7/dist-packages/konlpy/jvm.py, 67 line (convertStrings=True) comments processing and save jvm.py before import pakage\n",
        "\"\"\"\n",
        "\n",
        "\"\"\" \n",
        "ERROR 2. NameError: name 'Tagger' is not defined \n",
        "- Solution: Execute mecab.sh script (under code excute)\n",
        "\"\"\"\n",
        "!apt-get update\n",
        "!apt-get install g++ openjdk-8-jdk \n",
        "!pip3 install konlpy JPype1-py3\n",
        "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cu59D-1sc93c"
      },
      "source": [
        "# Import\n",
        "*   [KoNLPy Reperence](https://konlpy-ko.readthedocs.io/ko/v0.4.3/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRaXnlVOWpot"
      },
      "source": [
        "# import for MechineLearning\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "\n",
        "# import Morpheme analyzer\n",
        "from konlpy.tag import Kkma, Komoran, Okt, Mecab\n",
        "from konlpy.utils import pprint\n",
        "\n",
        "# import etc\n",
        "import time\n",
        "\n",
        "%cd /content/drive/My Drive/DeepLearning/AI_KKH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGm7Up9L5Y98"
      },
      "source": [
        "#Grobal Variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4l77b8v5aKe"
      },
      "source": [
        "AI_TARGET_NAME = \"프로젝트 김대엽 클럽장\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtyggGY871Bm"
      },
      "source": [
        "# Funtion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aaczn0GQ71N4"
      },
      "source": [
        "\"\"\"\n",
        "Function about time comparison\n",
        "\n",
        "def pos_times(taggers, tagger_name, texts) :\n",
        "    time_list = []\n",
        "    for tagger in taggers :\n",
        "        print(type(tagger))\n",
        "        \n",
        "        t1 = time.time()\n",
        "        for text in texts :\n",
        "            pprint(tagger.pos(text))\n",
        "        \n",
        "        print(\"###################################\")\n",
        "        t2 = time.time()\n",
        "        time_list.append(t2 - t1)\n",
        "\n",
        "    plt.figure(figsize=(10,8))\n",
        "    plt.bar(tagger_name, time_list, color=(0.4,0.7,0.5))\n",
        "    plt.title('Learning Time with .pos', fontsize=17)\n",
        "    plt.ylabel('total seconds')\n",
        "\"\"\"\n",
        "\n",
        "# Extract reply time (minute)\n",
        "def extract_replytime(content) :\n",
        "    end_idx = content.rfind(']')\n",
        "    start_idx = content[:end_idx].rfind('[') + 1\n",
        "\n",
        "    replytime = content[start_idx : end_idx]\n",
        "\n",
        "    if replytime[:2] == \"오전\" :\n",
        "        replytime = (int(replytime[2:replytime.index(\":\")]) * 60) + int(replytime[replytime.index(\":\") + 1 :])\n",
        "    else :\n",
        "        replytime = 60 * 12 + (int(replytime[2:replytime.index(\":\")]) * 60) + int(replytime[replytime.index(\":\") + 1 :])\n",
        "\n",
        "    return replytime\n",
        "\n",
        "# Comparison about reply time\n",
        "def compare_replytime(pre_text, cur_text) :\n",
        "    pre_time = extract_replytime(pre_text)\n",
        "    cur_time = extract_replytime(cur_text)\n",
        "\n",
        "    result = False\n",
        "    if cur_time - pre_time >= 10 : result = True\n",
        "\n",
        "    return result\n",
        "\n",
        "# Extract Data about kakao talk's content\n",
        "def extract_data(x_list, y_list) :\n",
        "    tmp_X_list = []\n",
        "    tmp_Y_list = []\n",
        "\n",
        "    before_respondent = \"\"\n",
        "    for text in test.values :\n",
        "        text = text.tolist()\n",
        "        for t in text : \n",
        "            if \":\" in t and before_respondent != \"\" :\n",
        "                cur_respondent = t[1:t.index(\"]\")]\n",
        "                if compare_replytime(text[text.index(t) - 1], t) or cur_respondent != before_respondent :\n",
        "                    if len(tmp_X_list) != 0 : x_list.append(tmp_X_list)\n",
        "                    if len(tmp_Y_list) != 0 : y_list.append(tmp_Y_list)\n",
        "\n",
        "                    tmp_X_list = []\n",
        "                    tmp_Y_list = []\n",
        "            if AI_TARGET_NAME in t :\n",
        "                ptext = t[(t.rfind(']') + 2) : len(t)]\n",
        "                if ptext.find(\"http\") == -1 and ptext != \"사진\" and ptext != \"\" : \n",
        "                    before_respondent = AI_TARGET_NAME\n",
        "                    tmp_Y_list.append(ptext)\n",
        "            elif \"]\" in t :\n",
        "                ptext = t[(t.rfind(']') + 2) : len(t)]\n",
        "                if ptext.find(\"http\") == -1 and ptext != \"사진\" and ptext != \"\" : \n",
        "                    before_respondent = cur_respondent\n",
        "                    tmp_X_list.append(ptext)\n",
        "\n",
        "# Auto calibrate spacing function\n",
        "def calibrate_spcaing(input_list) :\n",
        "    for idx in range(0, len(input_list)) :\n",
        "        for text in input_list[idx] :\n",
        "            analyzedRes = mecab.pos(text)\n",
        "            for mecabR in analyzedRes :\n",
        "                if mecabR[1] == \"MAG\" :\n",
        "                    try :\n",
        "                        startIdx = text.index(mecabR[0])\n",
        "                        if len(mecabR[0]) == 1 :\n",
        "                            if text[startIdx + 1] != \" \" and text[startIdx + 1].isalnum() : \n",
        "                                if text[startIdx + 2] != \" \" :\n",
        "                                    repairIdx = input_list[idx].index(text)\n",
        "                                    input_list[idx][repairIdx] = text[:startIdx] + text[startIdx] + \" \" + text[startIdx + 1 :]\n",
        "                        else : \n",
        "                            if text[startIdx + len(mecabR[0])] != \" \" and text[startIdx + len(mecabR[0])].isalnum() : \n",
        "                                if text[startIdx + len(mecabR[0]) + 1] != \" \" :\n",
        "                                    repairIdx = input_list[idx].index(text)\n",
        "                                    input_list[idx][repairIdx] = text[:startIdx] + text[startIdx : startIdx + len(mecabR[0])] + \" \" + text[startIdx + len(mecabR[0]) :]\n",
        "                    except IndexError :\n",
        "                        continue\n",
        "\n",
        "# Extraction 1 word by 1 content\n",
        "def extract_1word(input_list) :\n",
        "    for idx in range(0, len(input_list)) : \n",
        "        input_list[idx] = input_list[idx][0]\n",
        "\n",
        "# If x and y size different, equalize that\n",
        "def equalize_size(x, y) :\n",
        "    if len(x) > len(y) : x = x[0 : len(y)]\n",
        "    elif len(y) > len(x) : y = y[0 : len(x)]\n",
        "\n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxjmmKaVlO9p"
      },
      "source": [
        "#Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk3zCKMclQz2"
      },
      "source": [
        "\"\"\"\n",
        "ranking about time spent morpheme analyzing (The fastest is number one.)\n",
        "\n",
        "1. Mecab\n",
        "2. Komoran\n",
        "3. Okt\n",
        "4. Kkma\n",
        "\n",
        "Mecab is faster than Kkma about 30~40 times\n",
        "Mecab is faster than Okt about 10 times\n",
        "Mecab is faster than Komoran about 5 times\n",
        "\"\"\"\n",
        "\n",
        "# Early versions use okt and macab.\n",
        "mecab = Mecab()\n",
        "okt = Okt()\n",
        "\n",
        "\"\"\" \n",
        "ERROR 3. ParserError: Error tokenizing data. C error \n",
        "- Solution: add code in read_csv = , sep='\\t'\n",
        "\"\"\"\n",
        "test = pd.read_csv('KKH_20200129~20210707.txt', sep='\\t')\n",
        "\n",
        "# 1. Data extraction\n",
        "x_list = []\n",
        "y_list = []\n",
        "extract_data(x_list, y_list)\n",
        "\n",
        "# 2. Space cailbrating\n",
        "calibrate_spcaing(x_list)\n",
        "calibrate_spcaing(y_list)\n",
        "\n",
        "# 3. Data manufacturing\n",
        "# Early version is used y_list[][0] and x_list[][0] (1 word) by learning model\n",
        "extract_1word(x_list)\n",
        "extract_1word(y_list)\n",
        "# Size equalize\n",
        "x_list, y_list = equalize_size(x_list, y_list)\n",
        "# One-hot encoding\n",
        "\"\"\"\n",
        "# 4. Modeling\n",
        "model = Sequential()\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}